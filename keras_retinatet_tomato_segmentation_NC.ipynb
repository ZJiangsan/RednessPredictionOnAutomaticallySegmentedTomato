{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"keras_retinatet_tomato_segmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"gI1nyzeIyeKs","colab_type":"code","outputId":"dfc8bdb1-11bf-4270-ace0-e4106a835bfa","executionInfo":{"status":"ok","timestamp":1574617644079,"user_tz":-60,"elapsed":102812,"user":{"displayName":"Jiangping Zhao","photoUrl":"","userId":"12952536884307557281"}},"colab":{"base_uri":"https://localhost:8080/","height":137}},"source":["from google.colab import drive\n","import os\n","drive.mount('/content/drive', force_remount=True)\n","os.chdir(\"/content/drive/My Drive/app\") ## when you face error, \n","os.getcwd()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/app'"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"OXkqtIpdyfN6","colab_type":"code","outputId":"8b638835-b4c4-44f2-e802-91b7cf133456","executionInfo":{"status":"ok","timestamp":1574617667046,"user_tz":-60,"elapsed":20413,"user":{"displayName":"Jiangping Zhao","photoUrl":"","userId":"12952536884307557281"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install keras-retinanet\n","!pip install hdf5storage\n","!pip install keras-maskrcnn"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting keras-retinanet\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ca/63f77949493c63eebf982bc1edb0b54d849b6d709a724328ea5682e6b40e/keras-retinanet-0.5.1.tar.gz (61kB)\n","\r\u001b[K     |█████▍                          | 10kB 27.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████                | 30kB 4.3MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.1MB/s \n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (2.2.5)\n","Collecting keras-resnet\n","  Downloading https://files.pythonhosted.org/packages/76/d4/a35cbd07381139dda4db42c81b88c59254faac026109022727b45b31bcad/keras-resnet-0.2.0.tar.gz\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (1.12.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (1.3.2)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (0.29.14)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (4.3.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (3.4.7.28)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet) (3.38.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet) (1.17.4)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet) (1.0.8)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet) (1.1.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-retinanet) (3.13)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->keras-retinanet) (0.46)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet) (2.3.0)\n","Building wheels for collected packages: keras-retinanet, keras-resnet\n","  Building wheel for keras-retinanet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-retinanet: filename=keras_retinanet-0.5.1-cp36-cp36m-linux_x86_64.whl size=156768 sha256=c5b78cd4e07c2ea89937a44302005773b8142fcd5029263f938e761fda7097d7\n","  Stored in directory: /root/.cache/pip/wheels/d9/a5/98/87f8ed08913b184bb625dde1c1277a4dfe969c77b377abde69\n","  Building wheel for keras-resnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-resnet: filename=keras_resnet-0.2.0-py2.py3-none-any.whl size=20485 sha256=7943d1e839ef521669d449e481eeb83337c041d387e21775ace87d3e7f3243e6\n","  Stored in directory: /root/.cache/pip/wheels/5f/09/a5/497a30fd9ad9964e98a1254d1e164bcd1b8a5eda36197ecb3c\n","Successfully built keras-retinanet keras-resnet\n","Installing collected packages: keras-resnet, keras-retinanet\n","Successfully installed keras-resnet-0.2.0 keras-retinanet-0.5.1\n","Collecting hdf5storage\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/e0/5dd25068a231cd817265529368aca2f918049b290dcb2fd9b24ce136adf4/hdf5storage-0.1.15-py2.py3-none-any.whl (56kB)\n","\u001b[K     |████████████████████████████████| 61kB 3.2MB/s \n","\u001b[?25hRequirement already satisfied: h5py>=2.1; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (2.8.0)\n","Requirement already satisfied: numpy; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from hdf5storage) (1.17.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py>=2.1; python_version >= \"3.3\"->hdf5storage) (1.12.0)\n","Installing collected packages: hdf5storage\n","Successfully installed hdf5storage-0.1.15\n","Collecting keras-maskrcnn\n","  Downloading https://files.pythonhosted.org/packages/66/01/3a612c591c0ef1765af64b8e29b89e3ac2a9d89e720a6bba83f4090954d2/keras-maskrcnn-0.2.2.tar.gz\n","Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-maskrcnn) (2.2.5)\n","Requirement already satisfied: keras-retinanet in /usr/local/lib/python3.6/dist-packages (from keras-maskrcnn) (0.5.1)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (1.0.8)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (1.17.4)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (1.12.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (1.3.2)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-maskrcnn) (1.1.0)\n","Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->keras-maskrcnn) (0.29.14)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->keras-maskrcnn) (3.38.0)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->keras-maskrcnn) (3.4.7.28)\n","Requirement already satisfied: keras-resnet in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->keras-maskrcnn) (0.2.0)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from keras-retinanet->keras-maskrcnn) (4.3.0)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->keras-retinanet->keras-maskrcnn) (2.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->keras-retinanet->keras-maskrcnn) (0.46)\n","Building wheels for collected packages: keras-maskrcnn\n","  Building wheel for keras-maskrcnn (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-maskrcnn: filename=keras_maskrcnn-0.2.2-cp36-none-any.whl size=41692 sha256=cbd6d7af1bad97fefd1f4f6e8d2ce2b0e57b68fc4f1ec9a71b3abf1395fa70b5\n","  Stored in directory: /root/.cache/pip/wheels/0b/03/0e/652a317641021f3450b92b69567e64cc54597d71e809426be8\n","Successfully built keras-maskrcnn\n","Installing collected packages: keras-maskrcnn\n","Successfully installed keras-maskrcnn-0.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QAsO0oGyyfRD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ISZqeUAZbv7","colab_type":"code","colab":{}},"source":["import argparse\n","import os\n","import sys\n","import h5py\n","\n","import keras\n","import keras.preprocessing.image\n","import tensorflow as tf\n","\n","import keras_retinanet.losses\n","from keras_retinanet.callbacks import RedirectModel\n","from keras_retinanet.utils.config import read_config_file, parse_anchor_parameters\n","from keras_retinanet.utils.transform import random_transform_generator\n","from keras_retinanet.utils.keras_version import check_keras_version\n","from keras_retinanet.utils.model import freeze as freeze_model\n","\n","# Allow relative imports when being executed as script.\n","if __name__ == \"__main__\" and __package__ is None:\n","    sys.path.insert(0, os.path.join(os.path.dirname(\"__file__\"), '..', '..'))\n","    import keras_retinanet.bin  # noqa: F401\n","    __package__ = \"keras_maskrcnn.bin\"\n","\n","\n","# Change these to absolute imports if you copy this script outside the keras_retinanet package.\n","from .. import losses\n","from .. import models\n","from ..callbacks.eval import Evaluate"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DaEBYqAraUtD","colab_type":"code","colab":{}},"source":["def get_session():\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    return tf.Session(config=config)\n","\n","\n","def model_with_weights(model, weights, skip_mismatch):\n","    if weights is not None:\n","        model.load_weights(weights, by_name=True, skip_mismatch=skip_mismatch)\n","    return model\n","\n","\n","def create_models(backbone_retinanet, num_classes, weights, freeze_backbone=False, class_specific_filter=True, anchor_params=None):\n","    modifier = freeze_model if freeze_backbone else None\n","\n","    model            = model_with_weights(\n","        backbone_retinanet(\n","            num_classes,\n","            nms=True,\n","            class_specific_filter=class_specific_filter,\n","            modifier=modifier,\n","            anchor_params=anchor_params\n","        ), weights=weights, skip_mismatch=True)\n","    training_model   = model\n","    prediction_model = model\n","\n","    # compile model\n","    training_model.compile(\n","        loss={\n","            'regression'    : keras_retinanet.losses.smooth_l1(),\n","            'classification': keras_retinanet.losses.focal(),\n","            'masks'         : losses.mask(),\n","        },\n","        optimizer=keras.optimizers.adam(lr=2e-8, clipnorm=0.001)\n","    )\n","\n","    return model, training_model, prediction_model\n","\n","\n","def create_callbacks(model, training_model, prediction_model, validation_generator, args):\n","    callbacks = []\n","\n","    # save the prediction model\n","    if args.snapshots:\n","        # ensure directory created first; otherwise h5py will error after epoch.\n","        os.makedirs(args.snapshot_path, exist_ok=True)\n","        checkpoint = keras.callbacks.ModelCheckpoint(\n","            os.path.join(\n","                args.snapshot_path,\n","                '{backbone}_{dataset_type}_{{epoch:02d}}_new.h5'.format(backbone=args.backbone, dataset_type=args.dataset_type)\n","            ),\n","            verbose=1\n","        )\n","        checkpoint = RedirectModel(checkpoint, prediction_model)\n","        callbacks.append(checkpoint)\n","\n","    tensorboard_callback = None\n","\n","    if args.tensorboard_dir:\n","        tensorboard_callback = keras.callbacks.TensorBoard(\n","            log_dir                = args.tensorboard_dir,\n","            histogram_freq         = 0,\n","            batch_size             = args.batch_size,\n","            write_graph            = True,\n","            write_grads            = False,\n","            write_images           = False,\n","            embeddings_freq        = 0,\n","            embeddings_layer_names = None,\n","            embeddings_metadata    = None\n","        )\n","        callbacks.append(tensorboard_callback)\n","\n","    if args.evaluation and validation_generator:\n","        if args.dataset_type == 'coco':\n","            from ..callbacks.coco import CocoEval\n","\n","            # use prediction model for evaluation\n","            evaluation = CocoEval(validation_generator)\n","        else:\n","            evaluation = Evaluate(validation_generator, tensorboard=tensorboard_callback, weighted_average=args.weighted_average)\n","        evaluation = RedirectModel(evaluation, prediction_model)\n","        callbacks.append(evaluation)\n","\n","    callbacks.append(keras.callbacks.ReduceLROnPlateau(\n","        monitor  = 'loss',\n","        factor   = 0.1,\n","        patience = 2,\n","        verbose  = 1,\n","        mode     = 'auto',\n","        epsilon  = 0.0001,\n","        cooldown = 0,\n","        min_lr   = 0\n","    ))\n","\n","    return callbacks\n","\n","\n","def create_generators(args):\n","    # create random transform generator for augmenting training data\n","    transform_generator = random_transform_generator(min_rotation=-0.1,\n","            max_rotation=0.1,\n","            min_translation=(-0.1, -0.1),\n","            max_translation=(0.1, 0.1),\n","            min_shear=-0.1,\n","            max_shear=0.1,\n","            min_scaling=(0.5, 0.5),\n","            max_scaling=(1.1, 1.1),\n","            flip_x_chance=0.5,\n","            flip_y_chance=0.5,)\n","\n","    if args.dataset_type == 'coco':\n","        # import here to prevent unnecessary dependency on cocoapi\n","        from ..preprocessing.coco import CocoGenerator\n","\n","        train_generator = CocoGenerator(\n","            args.coco_path,\n","            'train2017',\n","            transform_generator=transform_generator,\n","            batch_size=args.batch_size,\n","            config=args.config\n","        )\n","\n","        validation_generator = CocoGenerator(\n","            args.coco_path,\n","            'val2017',\n","            batch_size=args.batch_size,\n","            config=args.config\n","        )\n","    elif args.dataset_type == 'csv':\n","        from ..preprocessing.csv_generator import CSVGenerator\n","\n","        train_generator = CSVGenerator(\n","            args.annotations,\n","            args.classes,\n","            transform_generator=random_transform_generator(min_rotation=-0.1,\n","            max_rotation=0.1,\n","            min_translation=(-0.1, -0.1),\n","            max_translation=(0.1, 0.1),\n","            min_shear=-0.1,\n","            max_shear=0.1,\n","            min_scaling=(0.4, 0.4),\n","            max_scaling=(1.1, 1.1),\n","            flip_x_chance=0.5,\n","            flip_y_chance=0.5,),\n","            batch_size=args.batch_size,\n","            config=args.config\n","        )\n","\n","        if args.val_annotations:\n","            validation_generator = CSVGenerator(\n","                args.val_annotations,\n","                args.classes,\n","                batch_size=args.batch_size,\n","                config=args.config\n","            )\n","        else:\n","            validation_generator = None\n","    else:\n","        raise ValueError('Invalid data type received: {}'.format(args.dataset_type))\n","\n","    return train_generator, validation_generator\n","\n","\n","def check_args(parsed_args):\n","    \"\"\"\n","    Function to check for inherent contradictions within parsed arguments.\n","    For example, batch_size < num_gpus\n","    Intended to raise errors prior to backend initialisation.\n","    :param parsed_args: parser.parse_args()\n","    :return: parsed_args\n","    \"\"\"\n","\n","    return parsed_args\n","\n","\n","def parse_args(args):\n","    parser     = argparse.ArgumentParser(description='Simple training script for training a RetinaNet mask network.')\n","    subparsers = parser.add_subparsers(help='Arguments for specific dataset types.', dest='dataset_type')\n","    subparsers.required = True\n","\n","    coco_parser = subparsers.add_parser('coco')\n","    coco_parser.add_argument('coco_path', help='Path to dataset directory (ie. /tmp/COCO).')\n","\n","    csv_parser = subparsers.add_parser('csv')\n","    csv_parser.add_argument('annotations', help='Path to CSV file containing annotations for training.')\n","    csv_parser.add_argument('classes', help='Path to a CSV file containing class label mapping.')\n","    csv_parser.add_argument('--val-annotations', help='Path to CSV file containing annotations for validation (optional).')\n","\n","    group = parser.add_mutually_exclusive_group()\n","    group.add_argument('--snapshot',          help='Resume training from a snapshot.')\n","    group.add_argument('--imagenet-weights',  help='Initialize the model with pretrained imagenet weights. This is the default behaviour.', action='store_const', const=True, default=True)\n","    group.add_argument('--weights',           help='Initialize the model with weights from a file.')\n","    group.add_argument('--no-weights',        help='Don\\'t initialize the model with any weights.', dest='imagenet_weights', action='store_const', const=False)\n","\n","    parser.add_argument('--backbone',         help='Backbone model used by retinanet.', default='resnet50', type=str)\n","    parser.add_argument('--batch-size',       help='Size of the batches.', default=1, type=int)\n","    parser.add_argument('--gpu',              help='Id of the GPU to use (as reported by nvidia-smi).')\n","    parser.add_argument('--epochs',           help='Number of epochs to train.', type=int, default=50)\n","    parser.add_argument('--steps',            help='Number of steps per epoch.', type=int, default=3000)\n","    parser.add_argument('--snapshot-path',    help='Path to store snapshots of models during training (defaults to \\'./snapshots\\')', default='./tomato_segmentation')\n","    parser.add_argument('--tensorboard-dir',  help='Log directory for Tensorboard output', default='./tomato_segmentation')\n","    parser.add_argument('--no-snapshots',     help='Disable saving snapshots.', dest='snapshots', action='store_false')\n","    parser.add_argument('--no-evaluation',    help='Disable per epoch evaluation.', dest='evaluation', action='store_false')\n","    parser.add_argument('--freeze-backbone',  help='Freeze training of backbone layers.', action='store_true')\n","    parser.add_argument('--no-class-specific-filter', help='Disables class specific filtering.', dest='class_specific_filter', action='store_false')\n","    parser.add_argument('--image-min-side',   help='Rescale the image so the smallest side is min_side.', type=int, default=668)\n","    parser.add_argument('--image-max-side',   help='Rescale the image if the largest side is larger than max_side.', type=int, default=890)\n","    parser.add_argument('--config',           help='Path to a configuration parameters .ini file.')\n","    parser.add_argument('--weighted-average', help='Compute the mAP using the weighted average of precisions among classes.', action='store_true')\n","\n","    # Fit generator arguments\n","    parser.add_argument('--workers', help='Number of multiprocessing workers. To disable multiprocessing, set workers to 0', type=int, default=1)\n","    parser.add_argument('--max-queue-size', help='Queue length for multiprocessing workers in fit generator.', type=int, default=10)\n","\n","    return check_args(parser.parse_args(args))\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ip32hbAk2-m","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuSuR-Le0vlS","colab_type":"code","colab":{}},"source":["args=check_args(parse_args([\"csv\",  os.getcwd()+\"/tomato_segmentation/annotation_tomato_segmentation.csv\", os.getcwd() +\"/tomato_segmentation/class_tomato.csv\"]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2QBaKnNGgVJY","colab_type":"text"},"source":["#Add this argument when you have trained model already"]},{"cell_type":"code","metadata":{"id":"pcu6JR_Zc_53","colab_type":"code","colab":{}},"source":["## add this argument when you have trained models and want to start based on them\n","args.snapshot=None\n","args.weights=os.getcwd()+\"/tomato_segmentation/resnet50_csv_09_new.h5\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFASDOMUggPb","colab_type":"text"},"source":["# prepare the model"]},{"cell_type":"code","metadata":{"id":"qOuWPl3G02LG","colab_type":"code","colab":{}},"source":["## prepare the model\n","\n","# make sure keras is the minimum required version\n","check_keras_version()\n","\n","# create object that stores backbone information\n","backbone = models.backbone(args.backbone)\n","\n","# optionally choose specific GPU\n","if args.gpu:\n","    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n","keras.backend.tensorflow_backend.set_session(get_session())\n","\n","# optionally load config parameters\n","if args.config:\n","    args.config = read_config_file(args.config)\n","\n","# create the generators\n","train_generator, validation_generator = create_generators(args)\n","\n","# create the model\n","if args.snapshot is not None:\n","    print('Loading model, this may take a second...')\n","    model            = models.load_model(args.snapshot, backbone_name=args.backbone)\n","    training_model   = model\n","    prediction_model = model\n","else:\n","    weights = args.weights\n","    print('Loading model and weights together, this may take several seconds...')\n","    # default to imagenet if nothing else is specified\n","    if weights is None and args.imagenet_weights:\n","        weights = backbone.download_imagenet()\n","\n","    anchor_params = None\n","    if args.config and 'anchor_parameters' in args.config:\n","        anchor_params = parse_anchor_parameters(args.config)\n","\n","    print('Creating model, this may take a second...')\n","    model, training_model, prediction_model = create_models(\n","        backbone_retinanet=backbone.maskrcnn,\n","        num_classes=train_generator.num_classes(),\n","        weights=weights,\n","        freeze_backbone=args.freeze_backbone,\n","        class_specific_filter=args.class_specific_filter,\n","        anchor_params=anchor_params\n","    )\n","\n","# print model summary\n","print(model.summary())\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8iybcXXHgktf","colab_type":"text"},"source":["# Train the model"]},{"cell_type":"code","metadata":{"id":"lK5kVjqx9OEB","colab_type":"code","outputId":"161527a3-1477-47a7-d8ad-7de7657f2af8","executionInfo":{"status":"error","timestamp":1572897825915,"user_tz":-60,"elapsed":25854481,"user":{"displayName":"jiangsan Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAoBwUnVvBMKk1dtsJwtnDFO6TQWaqdp4PFTMYbs74=s64","userId":"12734603351330672425"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# create the callbacks\n","callbacks = create_callbacks(\n","    model,\n","    training_model,\n","    prediction_model,\n","    validation_generator,\n","    args)\n","\n","# Use multiprocessing if workers > 0\n","if args.workers > 0:\n","    use_multiprocessing = True\n","else:\n","    use_multiprocessing = False\n","\n","# start training\n","training_model.fit_generator(\n","    generator=train_generator,\n","    steps_per_epoch=args.steps,\n","    epochs=args.epochs,\n","    verbose=1,\n","    callbacks=callbacks,\n","    workers=args.workers,\n","    use_multiprocessing=use_multiprocessing,\n","    max_queue_size=args.max_queue_size\n",")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1335: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n","  warnings.warn('`epsilon` argument is deprecated and '\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/50\n","   6/3000 [..............................] - ETA: 3:02:30 - loss: 0.4390 - regression_loss: 0.3253 - classification_loss: 0.0086 - masks_loss: 0.1050Epoch 1/50\n","3000/3000 [==============================] - 3550s 1s/step - loss: 0.2811 - regression_loss: 0.1843 - classification_loss: 0.0081 - masks_loss: 0.0887\n","\n","Epoch 00001: saving model to ./tomato_segmentation/resnet50_csv_01_new.h5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/50\n","3000/3000 [==============================] - 3552s 1s/step - loss: 0.2644 - regression_loss: 0.1690 - classification_loss: 0.0075 - masks_loss: 0.0879\n","\n","Epoch 00002: saving model to ./tomato_segmentation/resnet50_csv_02_new.h5\n","Epoch 3/50\n","3000/3000 [==============================] - 3524s 1s/step - loss: 0.2639 - regression_loss: 0.1688 - classification_loss: 0.0078 - masks_loss: 0.0873\n","\n","Epoch 00003: saving model to ./tomato_segmentation/resnet50_csv_03_new.h5\n","Epoch 4/50\n","3000/3000 [==============================] - 3528s 1s/step - loss: 0.2633 - regression_loss: 0.1679 - classification_loss: 0.0087 - masks_loss: 0.0868\n","\n","Epoch 00004: saving model to ./tomato_segmentation/resnet50_csv_04_new.h5\n","Epoch 5/50\n","3000/3000 [==============================] - 3513s 1s/step - loss: 0.2615 - regression_loss: 0.1665 - classification_loss: 0.0084 - masks_loss: 0.0866\n","\n","Epoch 00005: saving model to ./tomato_segmentation/resnet50_csv_05_new.h5\n","Epoch 6/50\n","3000/3000 [==============================] - 3512s 1s/step - loss: 0.2576 - regression_loss: 0.1633 - classification_loss: 0.0082 - masks_loss: 0.0862\n","\n","Epoch 00006: saving model to ./tomato_segmentation/resnet50_csv_06_new.h5\n","Epoch 7/50\n","3000/3000 [==============================] - 3479s 1s/step - loss: 0.2621 - regression_loss: 0.1666 - classification_loss: 0.0085 - masks_loss: 0.0870\n","\n","Epoch 00007: saving model to ./tomato_segmentation/resnet50_csv_07_new.h5\n","Epoch 8/50\n"," 697/3000 [=====>........................] - ETA: 44:36 - loss: 0.2750 - regression_loss: 0.1751 - classification_loss: 0.0121 - masks_loss: 0.0878"],"name":"stdout"},{"output_type":"stream","text":["Process ForkPoolWorker-3370:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n","    task = get()\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n","    res = self._reader.recv_bytes()\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n","    buf = self._recv_bytes(maxlength)\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n","    buf = self._recv(4)\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n","    chunk = read(handle, remaining)\n","KeyboardInterrupt\n"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-ef3ccfa25913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"0LogW9NUaU9c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d9sTI7U-aU_7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kzf8cnAwViJS","colab_type":"text"},"source":["# segment new **images**\n","### Load necessary ***modules***"]},{"cell_type":"code","metadata":{"id":"1C9BkAquaVCb","colab_type":"code","colab":{}},"source":["# import keras\n","import keras\n","\n","# import keras_retinanet\n","from keras_maskrcnn import models\n","from keras_maskrcnn.utils.visualization import draw_mask\n","from keras_retinanet.utils.visualization import draw_box, draw_caption, draw_annotations\n","from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n","from keras_retinanet.utils.colors import label_color\n","\n","# import miscellaneous modules\n","import matplotlib.pyplot as plt\n","import cv2\n","import os\n","import numpy as np\n","import time\n","\n","\n","# set tf backend to allow memory to grow, instead of claiming everything\n","import tensorflow as tf\n","\n","def get_session():\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","    return tf.Session(config=config)\n","\n","# set the modified tf session as backend in keras\n","keras.backend.tensorflow_backend.set_session(get_session())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Doh02YnIinJ","colab_type":"code","colab":{}},"source":["# here is to modify the draw_caption function\n","def draw_caption(image, box, caption):\n","    \"\"\" Draws a caption above the box in an image.\n","    # Arguments\n","        image   : The image to draw on.\n","        box     : A list of 4 elements (x1, y1, x2, y2).\n","        caption : String containing the text to draw.\n","    \"\"\"\n","    b = np.array(box).astype(int)\n","    cv2.putText(image, caption, (b[0]+20, b[1]+80), cv2.FONT_HERSHEY_DUPLEX, 0.6, (0, 0, 0), 2)\n","    cv2.putText(image, caption, (b[0]+20, b[1]+80), cv2.FONT_HERSHEY_DUPLEX, 0.6, (255, 255, 255), 1)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3qgamiWhV-tm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ubG3HihcVw5m","colab_type":"text"},"source":["### Load RetinaNet model"]},{"cell_type":"code","metadata":{"id":"KZ7Ea6z9aVGy","colab_type":"code","outputId":"f7723976-ce32-495d-85df-61f1964bdb7f","executionInfo":{"status":"ok","timestamp":1572897956603,"user_tz":-60,"elapsed":83088,"user":{"displayName":"jiangsan Zhao","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAoBwUnVvBMKk1dtsJwtnDFO6TQWaqdp4PFTMYbs74=s64","userId":"12734603351330672425"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# adjust this to point to your downloaded/trained model\n","model_path = os.path.join(os.getcwd(), 'tomato_segmentation', 'resnet50_csv_06_new.h5')## need to check and load the newest model weights\n","# load retinanet model\n","model = models.load_model(model_path, backbone_name='resnet50')\n","# load label to names mapping for visualization purposes\n","labels_to_names = {1: 'tomato'}"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:335: UserWarning: Output \"filtered_detections\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"filtered_detections\" during training.\n","  sample_weight_mode=sample_weight_mode)\n","/usr/local/lib/python3.6/dist-packages/keras/engine/saving.py:335: UserWarning: Output \"mask_submodel\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"mask_submodel\" during training.\n","  sample_weight_mode=sample_weight_mode)\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"6ElbPDXSV5NQ","colab_type":"text"},"source":["# Run detection on new image examples"]},{"cell_type":"code","metadata":{"id":"izlPu8zoJLMH","colab_type":"code","colab":{}},"source":["import cv2\n","from skimage.morphology import medial_axis\n","from math import sqrt\n","import matplotlib\n","from scipy.ndimage.interpolation import rotate\n","from matplotlib.colors import LinearSegmentedColormap\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aq7MYakVIefS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vkEr0YTuoow","colab_type":"code","colab":{}},"source":["import glob\n","## save masks, original image, and the overlapped images separately\n","im_path = 'image folder path'\n","redness_path = im_path.replace(\"Ori\", \"Pred\") \n","all_mask_all=[]\n","#for f in glob.glob(os.getcwd() + \"/stem_segmentation/ori_mask/*.jpg\"):\n","for f in glob.glob(im_path + \"/*.png\")[3:4]:\n","    \n","    id_=f.split(\"/\")[-1]\n","    print(id_)\n","    # load image\n","    image = cv2.imread(os.path.join(im_path, id_))\n","    if image.shape[0]<image.shape[1]:\n","        image =  cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n","    image = cv2.resize(image, (600,800)) #resize the image to match with the redness image\n","\n","    image= cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n","    # copy to draw on\n","    draw = image.copy()\n","    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n","    draw_redness = draw.copy()\n","    ##\n","    gray = cv2.cvtColor(draw_redness, cv2.COLOR_BGR2GRAY)\n","    blurred = cv2.GaussianBlur(gray, (5, 5), 1)\n","    thresh = cv2.threshold(blurred, 180, 255, cv2.THRESH_BINARY_INV)[1]\n","\n","    ###\n","    id_mat = id_.replace(\"png\", \"mat\")\n","    mat_i =  h5py.File(os.path.join(redness_path, id_mat),'r')\n","    redness_i =  np.float32(np.array(mat_i['img']))\n","    redness_i = rotate(redness_i, 90)\n","\n","    # preprocess image for network\n","    image = preprocess_image(image)\n","    image, scale = resize_image(image)\n","    \n","    # process image\n","    start = time.time()\n","    outputs = model.predict_on_batch(np.expand_dims(image, axis=0))\n","    print(\"processing time: \", time.time() - start)\n","    \n","    #print(outputs)\n","    boxes  = outputs[-4][0]\n","    scores = outputs[-3][0]\n","    labels = outputs[-2][0]\n","    masks  = outputs[-1][0]\n","    \n","    # correct for image scale\n","    boxes /= scale\n","    mask_all = []\n","    mask_all_bg=np.zeros(draw.shape[:2]) #.astype(np.uint8)\n","\n","    # visualize detections\n","    x=1\n","    for box, score, label, mask in zip(boxes, scores, labels, masks):\n","        print(\"score\", score)\n","        if score < 0.999:\n","            break\n","\n","        color = label_color(label)\n","        box_n = np.array([box[0]*1.01,box[1]*1.01,box[2]*0.99,box[3]*0.99]) ## make the box smaller remove the edge effects\n","        b = (box_n).astype(int)\n","        draw_box(draw, b, color=color)\n","        print(\"mask\", mask.shape)\n","        mask = mask[:, :, label]\n","        \n","        #print(mask.shape)\n","        draw_mask(draw, b, mask, color=label_color(label))\n","        ###############################################################\n","        # resize to fit the box\n","        mask = mask.astype(np.float32)\n","        mask = cv2.resize(mask, (b[2] - b[0], b[3] - b[1]))\n","    \n","        # binarize the mask\n","        mask = (mask > 0.7).astype(np.uint8)\n","\n","        print(redness_i.shape)\n","        print(b)\n","        mask_redness = redness_i[b[1]:b[3], b[0]:b[2]]\n","        print(\"mask_redness\",mask_redness.shape)\n","        redness_i_x = redness_i.copy()\n","        redness_i_x[b[1]:b[3], b[0]:b[2]] = 0  # mask the cropped part as 0\n","        mask_draw  = thresh[b[1]:b[3], b[0]:b[2]]\n","\n","        mask_draw[mask==0] = 0\n","\n","        mask_redness[mask_draw==0] = 0\n","        mask_redness_x = mask_redness.copy()\n","        mask_redness *=255\n","        #mask_redness = mask_redness.astype(np.uint8)\n","    \n","        print(\"mask_redness\", mask_redness.shape)\n","        label_redness_or = np.round(np.mean(mask_redness_x[mask_draw>0]), 3)\n","        label_redness = str(label_redness_or)\n","        print(\"label_redness\", label_redness)\n","\n","        mask_all_bg[b[1]:b[3], b[0]:b[2]] = mask_redness\n","        plt.figure(figsize=(6, 6))\n","        plt.axis('off')\n","        plt.imshow(mask_all_bg, cmap='RdYlGn')\n","        plt.show()\n","        ################################################################\n","        mask_all.append((\"{}_{}\".format(labels_to_names[label], str(x)), label_redness_or))\n","\n","        if label==1:\n","            caption = \"{}_{}\".format(labels_to_names[label],x)\n","            #draw_caption(draw, b, caption)\n","        else:\n","            caption = \"{}, Diameter={} cm\".format(labels_to_names[label],round(average_width,2))\n","    \n","        #print(caption)\n","        draw_caption(draw, b, caption)\n","        draw_caption(mask_all_bg, b, caption)  # second time, don't put tomato number\n","        b_x = b.copy()\n","        b_x[1]= b_x[1]+20\n","        draw_caption(mask_all_bg, b_x, label_redness)\n","        x += 1\n","        redness_i = redness_i_x\n","    all_mask_all.append((id_, mask_all))\n","    out_path = redness_path\n","    if not os.path.exists(out_path):\n","        os.makedirs(out_path)\n","\n","    plt.figure(figsize=(15, 15))\n","    plt.axis('off')\n","    plt.imshow(draw)\n","    plt.show()\n","    \n","    \n","    m = np.ma.masked_where((mask_all_bg==0), mask_all_bg)\n","    fig = plt.figure(figsize=(15, 15))\n","    ax = fig.add_subplot(111)\n","    ax.axis('off')\n","    im = ax.imshow(m/255, cmap='RdYlGn_r')\n","    cbar = fig.colorbar(im, shrink=0.8, aspect=20, fraction=.15, pad=.02)\n","    cbar.set_label('Redness',size=22)\n","    # access to cbar tick labels:\n","    cbar.ax.tick_params(labelsize=18)\n","    im.set_clim(0, 1) \n","    plt.savefig(out_path+\"/{}_{}\".format(\"tomato_redness_pred\", id_), bbox_inches='tight')\n","    \n","    img=cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n","\n","    cv2.imwrite(out_path+\"/{}_{}\".format(\"tomato_maskRCNN\", id_), img)\n","    #cv2.imwrite(out_path+\"/{}_{}\".format(\"maskRCNN_mask\",'1_3.jpg'), m)\n","    \n","with open(out_path +\"/tomato_redness.txt\", 'w') as f:\n","    f.write(\"%s\\n\" % all_mask_all)   \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UW1ZROe3-O0y","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OI8yZ4v09Wuq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAHav9aZaVYr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}